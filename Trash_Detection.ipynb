{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_IMmfdBuHOw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import yaml\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrlxN19Wj5KL",
        "outputId": "82a71519-db12-4171-9740-f9fa80083c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "replace /content/trash_dataset/README.dataset.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# Create a folder to extract\n",
        "!mkdir -p /content/trash_dataset\n",
        "# Unzip the uploaded file\n",
        "!unzip -q /content/trash.zip -d /content/trash_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDsWfFxauVXH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "images_dir = '/content/trash_dataset/train/images'\n",
        "labels_dir = '/content/trash_dataset/train/labels'\n",
        "\n",
        "# Get all image filenames\n",
        "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
        "\n",
        "# Create CSV data\n",
        "data = []\n",
        "for img_file in image_files:\n",
        "    label_file = os.path.splitext(img_file)[0] + '.txt'\n",
        "\n",
        "    img_path = os.path.join(images_dir, img_file)\n",
        "    label_path = os.path.join(labels_dir, label_file)\n",
        "\n",
        "    # Only include if label file exists\n",
        "    if os.path.exists(label_path):\n",
        "        data.append({'image_path': img_path, 'label_path': label_path})\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('annotations.csv', index=False)\n",
        "\n",
        "print(\"annotations.csv created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXGXkrXolzGD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yWkdb7AxKMx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Config ---\n",
        "csv_file = 'annotations.csv'\n",
        "dataset_root = 'dataset'\n",
        "train_ratio = 0.8             # 80% training, 20% validation\n",
        "\n",
        "# --- Make folders ---\n",
        "os.makedirs(f'{dataset_root}/images/train', exist_ok=True)\n",
        "os.makedirs(f'{dataset_root}/images/val', exist_ok=True)\n",
        "os.makedirs(f'{dataset_root}/labels/train', exist_ok=True)\n",
        "os.makedirs(f'{dataset_root}/labels/val', exist_ok=True)\n",
        "\n",
        "# --- Load CSV ---\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# --- Split CSV ---\n",
        "train_df, val_df = train_test_split(df, test_size=1-train_ratio, random_state=42)\n",
        "\n",
        "def copy_files(split_df, split):\n",
        "    for _, row in split_df.iterrows():\n",
        "        img_src = row['image_path']\n",
        "        lbl_src = row['label_path']\n",
        "\n",
        "        img_dst = os.path.join(dataset_root, 'images', split, os.path.basename(img_src))\n",
        "        lbl_dst = os.path.join(dataset_root, 'labels', split, os.path.basename(lbl_src))\n",
        "\n",
        "        shutil.copy(img_src, img_dst)\n",
        "        shutil.copy(lbl_src, lbl_dst)\n",
        "\n",
        "\n",
        "copy_files(train_df, 'train')\n",
        "copy_files(val_df, 'val')\n",
        "\n",
        "print(\" Images and labels copied to YOLO folder structure.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUhY7QPF9RdC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- CONFIG ---\n",
        "SRC_ROOT = \"dataset\"               # your original YOLO dataset (images/, labels/)\n",
        "DST_ROOT = \"dataset_5class\"        # new folder for merged dataset\n",
        "os.makedirs(f\"{DST_ROOT}/images/train\", exist_ok=True)\n",
        "os.makedirs(f\"{DST_ROOT}/images/val\", exist_ok=True)\n",
        "os.makedirs(f\"{DST_ROOT}/labels/train\", exist_ok=True)\n",
        "os.makedirs(f\"{DST_ROOT}/labels/val\", exist_ok=True)\n",
        "\n",
        "# --- CLASS MERGE MAP ---\n",
        "merge_map = {\n",
        "    # Plastic\n",
        "    \"plastic\": [\n",
        "        \"plastic\", \"plastic_bag\", \"plastic_container\", \"plastic_film\",\n",
        "        \"plastic_lid\", \"plastic_pack\", \"plastic_straw\", \"plastic_utensils\",\n",
        "        \"clear_plastic_bottle\", \"other_plastic\", \"other_plastic_bottle\",\n",
        "        \"other_plastic_wrapper\", \"single-use_carrier_bag\", \"styrofoam_piece\",\n",
        "        \"cups\", \"disposable_food_container\", \"disposable_plastic_cup\"\n",
        "    ],\n",
        "    # Metal\n",
        "    \"metal\": [\n",
        "        \"can\", \"drink_can\", \"metal\", \"foil\", \"aluminum_foil\",\n",
        "        \"pop_tab\", \"metal_bottle_cap\"\n",
        "    ],\n",
        "    # Paper / Cardboard\n",
        "    \"paper\": [\n",
        "        \"paper\", \"normal_paper\", \"carton\", \"tetra_pack\", \"paper_cup\",\n",
        "        \"drink_carton\", \"other_carton\"\n",
        "    ],\n",
        "    # Glass\n",
        "    \"glass\": [\n",
        "        \"glass_bottle\", \"other_glass\", \"jar\"\n",
        "    ],\n",
        "    # Other\n",
        "    \"other\": [\n",
        "        \"battery\", \"biological\", \"clothes\", \"clothing\", \"compostable\",\n",
        "        \"electronic\", \"rope&strings\", \"trash\", \"unknown\", \"wood\", \"tissues\", \"other\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Build lookup: fine class â†’ coarse class ID\n",
        "all_classes = sum(merge_map.values(), [])\n",
        "label_to_material = {}\n",
        "for i, (mat, items) in enumerate(merge_map.items()):\n",
        "    for c in items:\n",
        "        label_to_material[c] = i\n",
        "\n",
        "# --- Original class names (same as in your data.yaml) ---\n",
        "orig_class_names = [\n",
        "    \"aluminum_foil\", \"battery\", \"biological\", \"bottle\", \"can\", \"carton\",\n",
        "    \"cigarette\", \"clear_plastic_bottle\", \"clothes\", \"clothing\",\n",
        "    \"compostable\", \"cups\", \"disposable_food_container\",\n",
        "    \"disposable_plastic_cup\", \"drink_carton\", \"drink_can\",\n",
        "    \"electronic\", \"foil\", \"glass_bottle\", \"jar\", \"metal\",\n",
        "    \"metal_bottle_cap\", \"normal_paper\", \"other\", \"other_carton\",\n",
        "    \"other_glass\", \"other_plastic\", \"other_plastic_bottle\",\n",
        "    \"other_plastic_wrapper\", \"paper\", \"paper_cup\", \"plastic\",\n",
        "    \"plastic_bag\", \"plastic_container\", \"plastic_film\", \"plastic_lid\",\n",
        "    \"plastic_pack\", \"plastic_straw\", \"plastic_utensils\", \"pop_tab\",\n",
        "    \"rope&strings\", \"single-use_carrier_bag\", \"styrofoam_piece\",\n",
        "    \"tissues\", \"tetra_pack\", \"trash\", \"unknown\", \"wood\"\n",
        "]\n",
        "\n",
        "# --- Rewrite labels ---\n",
        "for split in [\"train\", \"val\"]:\n",
        "    src_img_dir = os.path.join(SRC_ROOT, \"images\", split)\n",
        "    src_lbl_dir = os.path.join(SRC_ROOT, \"labels\", split)\n",
        "    dst_img_dir = os.path.join(DST_ROOT, \"images\", split)\n",
        "    dst_lbl_dir = os.path.join(DST_ROOT, \"labels\", split)\n",
        "\n",
        "    for lbl_file in tqdm(os.listdir(src_lbl_dir), desc=f\"Processing {split}\"):\n",
        "        src_lbl_path = os.path.join(src_lbl_dir, lbl_file)\n",
        "        dst_lbl_path = os.path.join(dst_lbl_dir, lbl_file)\n",
        "\n",
        "        with open(src_lbl_path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "        new_lines = []\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls_id, xc, yc, w, h = parts\n",
        "            cls_name = orig_class_names[int(cls_id)]\n",
        "            if cls_name not in label_to_material:\n",
        "                continue\n",
        "            new_cls = label_to_material[cls_name]\n",
        "            new_lines.append(f\"{new_cls} {xc} {yc} {w} {h}\\n\")\n",
        "\n",
        "        if new_lines:\n",
        "            with open(dst_lbl_path, \"w\") as f:\n",
        "                f.writelines(new_lines)\n",
        "            # copy image only if it has valid labels\n",
        "            img_name = lbl_file.replace(\".txt\", \".jpg\")\n",
        "            shutil.copy(os.path.join(src_img_dir, img_name), os.path.join(dst_img_dir, img_name))\n",
        "\n",
        "# --- Write new data.yaml ---\n",
        "data_yaml = {\n",
        "    'path': os.path.abspath(DST_ROOT),\n",
        "    'train': 'images/train',\n",
        "    'val': 'images/val',\n",
        "    'names': {0: \"plastic\", 1: \"metal\", 2: \"paper\", 3: \"glass\", 4: \"other\"}\n",
        "}\n",
        "yaml_path = os.path.join(DST_ROOT, 'data.yaml')\n",
        "with open(yaml_path, 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(\"\\nâœ… New 5-class dataset created at:\", DST_ROOT)\n",
        "print(\"âœ… You can now train YOLO using this file:\", yaml_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Z3CiXZ5eoT5S"
      },
      "outputs": [],
      "source": [
        "pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zP0qZe1Tpqbe"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Path to your dataset config file (data.yaml)\n",
        "data_yaml = r\"/content/dataset_5class/data.yaml\"\n",
        "\n",
        "# --- Model setup ---\n",
        "model_version = 'yolov8'   # no comma here!\n",
        "variants = ['s']           # or ['m'] if GPU allows\n",
        "\n",
        "for variant in variants:\n",
        "    model_name = f'{model_version}{variant}.pt'\n",
        "    model = YOLO(model_name)  # Load pre-trained YOLO model\n",
        "\n",
        "    # --- Training ---\n",
        "    model.train(\n",
        "        data=data_yaml,        # path to data.yaml\n",
        "        epochs=50,             # train longer for better accuracy\n",
        "        imgsz=640,             # image size\n",
        "        batch=16,              # adjust if memory issue\n",
        "        name=f'{model_version}{variant}_trained',\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Training complete for {model_name}\")\n",
        "\n",
        "print(\"ðŸŽ¯ All models trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROHpvDc--q6Q"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# load your trained model\n",
        "model = YOLO(\"/content/runs/detect/yolov8s_trained/weights/best.pt\")\n",
        "\n",
        "# test on a folder of images (e.g., test set)\n",
        "results = model.predict(source=\"/content/trash_dataset/test/images\", save=True, imgsz=640)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUL9oZCZDuMe"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import Video\n",
        "# Load your trained model\n",
        "model = YOLO(\"yolov8s.pt\")\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from IPython.display import display, Image, clear_output\n",
        "import time\n",
        "\n",
        "# Path to uploaded video\n",
        "video_path = \"/content/video.mp4\"\n",
        "\n",
        "# Run prediction and save annotated video\n",
        "results = model.predict(source=video_path, save=True)  # saves in runs/detect/predict\n",
        "\n",
        "# Get path of saved video\n",
        "output_video = results[0].path\n",
        "\n",
        "# Display the video in Colab\n",
        "Video(output_video, embed=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NewUAzuTBwz3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}